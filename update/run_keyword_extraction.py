# run_keyword_extraction.py
"""File n√†y s·∫Ω thay th·∫ø cho logic x·ª≠ l√Ω d·ªØ li·ªáu th√¥ trong c√°c file run.py v√† keyword_save_es.py c≈©. 
Nhi·ªám v·ª• duy nh·∫•t c·ªßa n√≥ l√† ƒë·ªçc d·ªØ li·ªáu b√†i b√°o m·ªõi, d√πng py_vncorenlp ƒë·ªÉ tr√≠ch xu·∫•t t·ª´ kh√≥a v√† 
l∆∞u v√†o Elasticsearch v·ªõi c·∫•u tr√∫c chu·∫©n. ƒê√¢y l√† m·ªôt b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω, c·∫ßn ƒë∆∞·ª£c ch·∫°y m·ªói khi c√≥ d·ªØ li·ªáu m·ªõi."""


import json
from datetime import datetime
from elasticsearch import Elasticsearch, helpers
import py_vncorenlp

# --- C·∫§U H√åNH ---
ES_HOST = "localhost"
ES_PORT = 9200
INDEX_NAME = "newspaper_articles_with_keywords" # Index ƒë·ªÉ l∆∞u k·∫øt qu·∫£
VNCORENLP_MODEL_PATH = "C:/path/to/your/vncorenlp" # ƒê∆∞·ªùng d·∫´n ƒë·∫øn model VnCoreNLP c·ªßa b·∫°n

# --- K·∫æT N·ªêI ---
es_client = Elasticsearch([{'host': ES_HOST, 'port': ES_PORT, 'scheme': 'http'}])
rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=["wseg", "pos", "ner"], save_dir=VNCORENLP_MODEL_PATH)

def create_index_if_not_exists(client, index):
    """T·∫°o index v·ªõi mapping ph√π h·ª£p n·∫øu ch∆∞a t·ªìn t·∫°i."""
    mapping = {
        "properties": {
            "title": {"type": "text"},
            "content": {"type": "text"},
            "author": {"type": "keyword"},
            "created_time": {"type": "date"},
            "keywords": {"type": "keyword"} # Quan tr·ªçng: ƒë·ªÉ 'keyword' ƒë·ªÉ aggregate
        }
    }
    if not client.indices.exists(index=index):
        client.indices.create(index=index, mappings=mapping)
        print(f"‚úÖ Index '{index}' ƒë√£ ƒë∆∞·ª£c t·∫°o.")

def extract_keywords(text):
    """
    S·ª≠ d·ª•ng VnCoreNLP ƒë·ªÉ tr√≠ch xu·∫•t c√°c c·ª•m danh t·ª´ (Noun Phrases)
    v√† th·ª±c th·ªÉ c√≥ t√™n (Named Entities) l√†m t·ª´ kh√≥a.
    """
    if not text or not isinstance(text, str):
        return []

    annotations = rdrsegmenter.annotate_text(text)
    keywords = set() # D√πng set ƒë·ªÉ tr√°nh tr√πng l·∫∑p

    for sentence in annotations:
        for word in sentence:
            # L·∫•y c√°c c·ª•m danh t·ª´, th·ª±c th·ªÉ t√™n ri√™ng
            # V√≠ d·ª•: Np (Proper Noun), Nc (Common Noun), B-PER (Person), B-LOC (Location)...
            if word['posTag'].startswith('N') or word['nerLabel'].startswith('B-'):
                keywords.add(word['wordForm'].replace(" ", "_"))
    return list(keywords)

def process_and_index_data(input_file_path):
    """
    ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON, tr√≠ch xu·∫•t t·ª´ kh√≥a v√† ƒë·∫©y v√†o Elasticsearch.
    """
    actions = []
    with open(input_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                article = json.loads(line)
                # Gi·∫£ s·ª≠ b√†i b√°o c√≥ c√°c tr∆∞·ªùng 'title', 'content', 'created_time'
                text_to_analyze = article.get('title', '') + " " + article.get('content', '')
                keywords = extract_keywords(text_to_analyze)

                # Chu·∫©n b·ªã document ƒë·ªÉ index
                document = {
                    "_index": INDEX_NAME,
                    "_source": {
                        "title": article.get('title'),
                        "content": article.get('content'),
                        "author": article.get('author'),
                        # Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng ng√†y th√°ng n·∫øu c·∫ßn
                        "created_time": datetime.strptime(article.get('created_time'), "%d/%m/%Y %H:%M").isoformat(),
                        "keywords": keywords
                    }
                }
                actions.append(document)

                # ƒê·∫©y d·ªØ li·ªáu theo batch ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t
                if len(actions) >= 500:
                    helpers.bulk(es_client, actions)
                    print(f"ƒê√£ index {len(actions)} b·∫£n ghi...")
                    actions = []

            except (json.JSONDecodeError, KeyError) as e:
                print(f"‚ö†Ô∏è B·ªè qua d√≤ng l·ªói: {line.strip()} - L·ªói: {e}")
                continue

    # Index n·ªët ph·∫ßn d·ªØ li·ªáu c√≤n l·∫°i
    if actions:
        helpers.bulk(es_client, actions)
        print(f"ƒê√£ index {len(actions)} b·∫£n ghi cu·ªëi c√πng.")

    print("üéâ Ho√†n t·∫•t qu√° tr√¨nh x·ª≠ l√Ω v√† index d·ªØ li·ªáu.")


if __name__ == "__main__":
    # 1. ƒê·∫£m b·∫£o index ƒë√£ t·ªìn t·∫°i
    create_index_if_not_exists(es_client, INDEX_NAME)

    # 2. Cung c·∫•p ƒë∆∞·ªùng d·∫´n ƒë·∫øn file d·ªØ li·ªáu th√¥ c·ªßa b·∫°n
    INPUT_DATA_FILE = "path/to/your/articles.json"
    process_and_index_data(INPUT_DATA_FILE)